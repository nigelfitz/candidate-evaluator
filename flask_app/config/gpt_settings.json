{
  "_metadata": {
    "description": "Two-Agent AI Configuration for Candidate Evaluation System",
    "last_updated": "2026-01-08 02:08:57 UTC",
    "version": "2.0",
    "warning": "Changes to these settings affect both quality and cost. Test with small batches first.",
    "architecture": "RANKER_AGENT handles bulk scoring (all candidates × all criteria). INSIGHT_AGENT generates deep insights for selected candidates only."
  },
  "ranker_model": {
    "value": "gpt-4o-mini",
    "description": "Model for RANKER_AGENT - handles bulk scoring and criteria extraction",
    "options": [
      "gpt-4o-mini",
      "gpt-4o",
      "gpt-4-turbo",
      "gpt-4"
    ],
    "explanation": "gpt-4o-mini: RECOMMENDED for ranker. 10x cheaper than gpt-4o, sufficient quality for scoring. gpt-4o: Higher quality but expensive for bulk work. Use only if quality issues with mini.",
    "cost_impact": "gpt-4o-mini: $0.15 per 1M input, $0.60 per 1M output. gpt-4o: $2.50/$10. For 50 candidates × 20 criteria (1000 calls), mini=$0.30, gpt-4o=$3.00.",
    "quality_impact": "gpt-4o-mini provides reliable 0-100 scores and justifications. gpt-4o offers marginally better evidence extraction but rarely worth 10x cost for bulk scoring.",
    "usage": "Called (# candidates × # criteria) times per analysis. This is your highest-volume operation."
  },
  "insight_model": {
    "value": "gpt-4o",
    "description": "Model for INSIGHT_AGENT - generates deep insights for selected candidates",
    "options": [
      "gpt-4o",
      "gpt-4o-mini",
      "gpt-4-turbo",
      "gpt-4"
    ],
    "explanation": "gpt-4o: RECOMMENDED for insights. Best readability and personalization. gpt-4o-mini: 60% cheaper but generic phrasing. gpt-4-turbo: Same cost as gpt-4o, similar quality. gpt-4: More expensive, no benefit.",
    "cost_impact": "gpt-4o: ~$0.03 per insight (3000 tokens in, 800 tokens out). gpt-4o-mini: ~$0.01 per insight. For 5 insights, difference is $0.10.",
    "quality_impact": "gpt-4o produces professional, specific insights with natural phrasing. gpt-4o-mini works but tends toward generic language ('demonstrates strong skills...').",
    "usage": "Called only for candidates you select for 'Deep Insights' (typically top 3-10). Low volume, high value."
  },
  "ranker_temperature": {
    "value": 0.0,
    "description": "Temperature for RANKER_AGENT (0.0 = deterministic, 2.0 = creative)",
    "range": [
      0.0,
      2.0
    ],
    "explanation": "RANKER needs consistency across thousands of scoring calls. 0.1 ensures same resume+criterion always gets same score. Higher values introduce randomness.",
    "recommended": 0.1,
    "cost_impact": "No cost impact - only affects consistency",
    "quality_impact": "0.1 = highly consistent scoring. 0.3+ = scores may vary on re-run. Keep LOW for ranker."
  },
  "insight_temperature": {
    "value": 0.4,
    "description": "Temperature for INSIGHT_AGENT (0.0 = deterministic, 2.0 = creative)",
    "range": [
      0.0,
      2.0
    ],
    "explanation": "INSIGHT benefits from readability and natural phrasing. 0.4 balances consistency with engaging language. Too low (0.1) = robotic. Too high (0.8) = inconsistent.",
    "recommended": 0.4,
    "cost_impact": "No cost impact - only affects writing style",
    "quality_impact": "0.4 produces professional, readable insights without excessive creativity. Sweet spot for hiring assessments."
  },
  "advanced_api_settings": {
    "presence_penalty": {
      "value": 0.4,
      "description": "Reduces repetitive phrasing across insights (0.0 = no penalty, 2.0 = max penalty)",
      "range": [
        0.0,
        2.0
      ],
      "explanation": "Penalizes using the same phrases repeatedly across multiple candidates. Helps avoid 'demonstrates strong...' appearing in every insight.",
      "recommended": 0.4,
      "applies_to": "INSIGHT_AGENT only",
      "cost_impact": "No cost impact - only affects output style",
      "quality_impact": "0.3-0.6 produces more personalized, varied insights. Too high (>1.0) may produce awkward phrasing."
    },
    "frequency_penalty": {
      "value": 0.3,
      "description": "Discourages repeating words within a single response (0.0 = no penalty, 2.0 = max penalty)",
      "range": [
        0.0,
        2.0
      ],
      "explanation": "Reduces word repetition within each insight. Prevents 'Python experience... experienced with Python... Python skills' redundancy.",
      "recommended": 0.3,
      "applies_to": "INSIGHT_AGENT only",
      "cost_impact": "No cost impact - only affects output style",
      "quality_impact": "0.2-0.4 produces more natural, varied language. Too high (>0.8) may lose important keyword emphasis."
    },
    "ranker_max_tokens": {
      "value": 300,
      "description": "Max tokens for RANKER responses (score + justification + evidence)",
      "range": [
        200,
        500
      ],
      "explanation": "RANKER returns: score (integer), justification (1 sentence), raw_evidence (verbatim quotes). 300 tokens = ~225 words, sufficient for detailed evidence.",
      "recommended": 300,
      "cost_impact": "Each token costs $0.0000006 (gpt-4o-mini). 300 tokens = $0.00018 per call. For 1000 calls, difference between 200 and 300 tokens is $0.06.",
      "quality_impact": "300 tokens ensures complete evidence extraction. 200 may truncate quotes. 500 is overkill and wastes money."
    },
    "insight_max_tokens": {
      "value": 1500,
      "description": "Max tokens for INSIGHT responses (strengths + gaps + notes + justifications)",
      "range": [
        500,
        2000
      ],
      "explanation": "INSIGHT returns: 3-6 strength bullets, 3-6 gap bullets, 2-4 sentence notes, plus justifications dict. With larger context windows, need more output tokens. Typical output: 600-1000 tokens. 1500 provides safety margin.",
      "recommended": 1500,
      "cost_impact": "Each token costs $0.00001 (gpt-4o). 1500 tokens = $0.015 per insight. 2000 tokens = $0.02. Increased from 1000 to handle justifications dict.",
      "quality_impact": "Too low (<800) may truncate JSON response mid-string. 1500 ensures complete response with justifications. 2000 is maximum safety but usually unnecessary."
    }
  },
  "text_processing": {
    "candidate_text_chars": {
      "value": 25000,
      "description": "Maximum resume text sent to AI (character limit)",
      "range": [
        4000,
        20000
      ],
      "explanation": "Truncates long resumes to control token costs. Most resumes are 2-5k chars. 12000 captures comprehensive context.",
      "recommended": 12000,
      "cost_impact": "Each 1000 chars ≈ 250 tokens. 12000 chars = 3000 tokens input per scoring call. For 1000 RANKER calls: 12k=$0.45, 20k=$0.75.",
      "quality_impact": "Too low (<6000) may miss relevant experience. 12000 captures full career history. 20000 only needed for extremely detailed CVs (rare)."
    },
    "jd_text_chars": {
      "value": 15000,
      "description": "Maximum JD text sent to AI (character limit)",
      "range": [
        1000,
        15000
      ],
      "explanation": "Truncates long job descriptions. 5000 chars captures comprehensive requirements without excessive context.",
      "recommended": 5000,
      "cost_impact": "Each 1000 chars ≈ 250 tokens. 5000 chars = 1250 tokens per call. Moderate impact on total cost.",
      "quality_impact": "Too low (<2000) may miss important requirements. 5000 captures detailed JDs. 15000 for exceptionally comprehensive job descriptions."
    },
    "evidence_snippet_chars": {
      "value": 500,
      "description": "Maximum characters per evidence snippet displayed in UI",
      "range": [
        300,
        1000
      ],
      "explanation": "Controls how much of the raw_evidence text is shown to users. Doesn't affect AI processing, only UI display.",
      "recommended": 500,
      "cost_impact": "No cost impact - UI only",
      "quality_impact": "Too low (<400) may truncate context. 500 shows adequate proof. 1000 can be overwhelming."
    }
  },
  "insight_formatting": {
    "notes_length": {
      "value": "concise",
      "description": "Length of the 'Overall Notes' section in candidate insights",
      "options": [
        "brief",
        "concise",
        "detailed"
      ],
      "explanation": "Controls how much detail the AI includes in the overall assessment notes. Brief=1-2 sentences, Concise=2-4 sentences, Detailed=4-6 sentences (one paragraph).",
      "recommended": "concise",
      "cost_impact": "Minimal - only affects INSIGHT output length. Brief saves ~50 tokens per insight, Detailed adds ~100 tokens. For 5 insights: Brief=$0.005 savings, Detailed=$0.01 extra cost.",
      "quality_impact": "Brief may feel rushed. Concise balances detail and readability. Detailed provides more context but can be verbose."
    },
    "insight_tone": {
      "value": "professional",
      "description": "Writing style and tone for generated insights",
      "options": [
        "professional",
        "conversational",
        "technical",
        "executive"
      ],
      "explanation": "Professional: Standard hiring language. Conversational: Friendly, approachable tone. Technical: Tech-focused with specific details. Executive: High-level strategic assessment for C-suite roles.",
      "recommended": "professional",
      "cost_impact": "No cost impact - only affects phrasing style",
      "quality_impact": "Professional works for most roles. Technical better for engineering/dev roles. Executive better for senior leadership. Conversational for startup/casual environments."
    }
  },
  "report_balance": {
    "min_strengths": {
      "value": 3,
      "description": "Minimum number of strengths to generate per candidate",
      "range": [
        1,
        6
      ],
      "explanation": "Controls minimum length of 'Top Strengths' section. Lower values allow more concise reports. Higher values ensure comprehensive coverage.",
      "recommended": 3,
      "cost_impact": "Minimal - affects INSIGHT output length. Each strength ~30 tokens. 3 strengths = 90 tokens, 6 strengths = 180 tokens. Cost difference: ~$0.001 per insight.",
      "quality_impact": "Too low (<3) may miss important strengths. 3-4 is balanced. 5-6 provides exhaustive coverage but can be redundant."
    },
    "max_strengths": {
      "value": 6,
      "description": "Maximum number of strengths to generate per candidate",
      "range": [
        3,
        10
      ],
      "explanation": "Controls maximum length of 'Top Strengths' section. Prevents overly long reports.",
      "recommended": 6,
      "cost_impact": "Higher max increases output tokens. 6 strengths = 180 tokens, 10 strengths = 300 tokens. Cost difference: ~$0.001-0.002 per insight.",
      "quality_impact": "6 is optimal for most roles. 8-10 useful for complex senior roles with many requirements."
    },
    "min_gaps": {
      "value": 3,
      "description": "Minimum number of gaps/concerns to generate per candidate",
      "range": [
        1,
        6
      ],
      "explanation": "Controls minimum length of 'Gaps & Risks' section. Ensures concerns are flagged.",
      "recommended": 3,
      "cost_impact": "Minimal - affects INSIGHT output length. Each gap ~30 tokens. 3 gaps = 90 tokens, 6 gaps = 180 tokens. Cost difference: ~$0.001 per insight.",
      "quality_impact": "Too low (<3) may miss red flags. 3-4 is balanced. 5-6 provides thorough risk assessment."
    },
    "max_gaps": {
      "value": 6,
      "description": "Maximum number of gaps/concerns to generate per candidate",
      "range": [
        3,
        10
      ],
      "explanation": "Controls maximum length of 'Gaps & Risks' section. Prevents overwhelming negative feedback.",
      "recommended": 6,
      "cost_impact": "Higher max increases output tokens. 6 gaps = 180 tokens, 10 gaps = 300 tokens. Cost difference: ~$0.001-0.002 per insight.",
      "quality_impact": "6 is optimal. Too many gaps can feel overly critical. 8-10 only for extremely mismatched candidates."
    }
  },
  "evidence_depth": {
    "top_evidence_items": {
      "value": 5,
      "description": "Number of evidence items to show per criterion in detailed view",
      "range": [
        1,
        10
      ],
      "explanation": "Controls how many evidence snippets are displayed per criterion in the UI. Higher values provide more context but can overwhelm.",
      "recommended": 5,
      "cost_impact": "No direct cost impact - UI only. Does not affect AI processing.",
      "quality_impact": "3 is minimal. 5 provides good context. 7-10 can be excessive for most criteria."
    }
  },
  "evidence_thresholds": {
    "strong_match_threshold": {
      "value": 0.75,
      "description": "Score threshold for 'STRONG match' classification",
      "range": [
        0.6,
        0.9
      ],
      "explanation": "Scores at or above this value are classified as 'STRONG match' (75%+ by default). Affects AI's tone when writing justifications and how evidence is labeled in UI.",
      "recommended": 0.75,
      "cost_impact": "No cost impact - only affects classification logic",
      "quality_impact": "Too low (<0.70) dilutes 'strong' meaning. 0.75 is balanced. Too high (>0.80) makes strong matches rare."
    },
    "good_match_threshold": {
      "value": 0.5,
      "description": "Score threshold for 'GOOD match' classification",
      "range": [
        0.4,
        0.7
      ],
      "explanation": "Scores between this and strong_threshold are 'GOOD match' (50-74% by default).",
      "recommended": 0.5,
      "cost_impact": "No cost impact - only affects classification logic",
      "quality_impact": "0.50 is balanced. Too low (<0.45) makes 'good' too easy. Too high (>0.60) makes moderate matches rare."
    },
    "moderate_match_threshold": {
      "value": 0.35,
      "description": "Score threshold for 'MODERATE match' classification",
      "range": [
        0.25,
        0.5
      ],
      "explanation": "Scores between this and good_threshold are 'MODERATE match' (35-49% by default).",
      "recommended": 0.35,
      "cost_impact": "No cost impact - only affects classification logic",
      "quality_impact": "0.35 is balanced. Adjust based on how strict you want the 'moderate' band to be."
    },
    "weak_match_threshold": {
      "value": 0.15,
      "description": "Score threshold for 'WEAK match' classification",
      "range": [
        0.1,
        0.3
      ],
      "explanation": "Scores between this and moderate_threshold are 'WEAK match' (15-34% by default). Below this is 'MINIMAL match'.",
      "recommended": 0.15,
      "cost_impact": "No cost impact - only affects classification logic",
      "quality_impact": "0.15 is balanced. Below this, evidence is truly minimal or absent."
    }
  },
  "score_thresholds": {
    "high_threshold": {
      "value": 0.75,
      "description": "Score ≥75% = 'Strong Match' (green in UI)",
      "range": [
        0.6,
        0.9
      ],
      "explanation": "Criteria scoring 75+ are highlighted as strengths. Lower threshold = more 'strengths'. Higher = more selective.",
      "recommended": 0.75
    },
    "low_threshold": {
      "value": 0.35,
      "description": "Score <35% = 'Weak Match' (red in UI)",
      "range": [
        0.2,
        0.5
      ],
      "explanation": "Criteria scoring below 35 are flagged as gaps. Lower threshold = fewer gaps. Higher = more critical assessment.",
      "recommended": 0.35
    }
  },
  "pricing_config": {
    "standard_tier_price": {
      "value": 10.0,
      "description": "Price for Standard tier analysis (full scoring + top 3 insights)",
      "explanation": "Covers: JD extraction, scoring all candidates × all criteria, generating 3 insights. Includes profit margin over OpenAI costs."
    },
    "deep_dive_unlock_price": {
      "value": 1.0,
      "description": "Price to unlock Deep Insights for one additional candidate",
      "explanation": "Each additional insight costs ~$0.03 in OpenAI fees. $1.00 provides healthy profit margin while staying affordable."
    },
    "model_costs": {
      "gpt-4o": {
        "input_per_million": 2.5,
        "output_per_million": 10.0
      },
      "gpt-4o-mini": {
        "input_per_million": 0.15,
        "output_per_million": 0.6
      },
      "gpt-4-turbo": {
        "input_per_million": 10.0,
        "output_per_million": 30.0
      },
      "gpt-4": {
        "input_per_million": 30.0,
        "output_per_million": 60.0
      }
    }
  },
  "calculator_settings": {
    "avg_candidates_per_job": {
      "value": 50,
      "description": "Average number of candidates per job for cost estimation",
      "range": [
        10,
        200
      ],
      "explanation": "Used by Business Health Monitor to estimate typical analysis costs. Adjust to match your typical job posting volume."
    },
    "avg_criteria_per_job": {
      "value": 20,
      "description": "Average number of criteria per job description",
      "range": [
        5,
        40
      ],
      "explanation": "Typical JD has 15-25 criteria. More criteria = more RANKER calls = higher cost. Formula: candidates × criteria = ranker_calls."
    },
    "insights_generated": {
      "value": 5,
      "description": "Number of insights generated per analysis (Standard tier)",
      "range": [
        3,
        10
      ],
      "explanation": "Each insight costs ~$0.03 with gpt-4o. Standard tier includes 3-5 insights. User can unlock more."
    },
    "standard_tier_revenue": {
      "value": 10.0,
      "description": "Revenue per Standard tier analysis",
      "range": [
        5.0,
        20.0
      ],
      "explanation": "Current pricing: $10.00 for full analysis with 3-5 insights. Calculator shows profit margin at this price point."
    },
    "token_estimation_formulas": {
      "ranker_input_formula": "(jd_text_chars + candidate_text_chars) / 4",
      "insight_input_formula": "(jd_text_chars + candidate_text_chars + (criteria_count * 500)) / 4",
      "explanation": "Token estimation: ~4 characters per token. RANKER gets JD + candidate. INSIGHT gets JD + candidate + evidence (500 chars per criterion)."
    }
  },
  "_usage_notes": {
    "current_architecture": "TWO-AGENT SYSTEM: RANKER (bulk scoring) + INSIGHT (deep analysis for selected candidates)",
    "typical_analysis_cost": "50 candidates, 20 criteria: RANKER costs vary with text limits. With 12k+5k chars: ranker input ~4250 tokens, insight input ~5000 tokens. See Calculator Settings for live estimates.",
    "recommended_workflow": "1. Test changes with small job (10 candidates). 2. Check quality in Insights page. 3. Monitor costs in Stats page. 4. Adjust gradually.",
    "optimization_priority": "1. Keep RANKER=gpt-4o-mini (already optimized). 2. Keep INSIGHT=gpt-4o (quality matters here). 3. Adjust temperatures only if quality issues. 4. Don't change max_tokens unless truncation occurs.",
    "cost_saving_tips": "For budget jobs: Use gpt-4o-mini for both agents (saves 60% on insights). Reduce ranker_max_tokens to 200 (saves 33%). Skip Deep Insights for weak candidates.",
    "quality_improvement_tips": "If scores seem inconsistent: Lower ranker_temperature to 0.05. If insights seem generic: Increase insight_temperature to 0.5. If evidence incomplete: Raise ranker_max_tokens to 400."
  }
}