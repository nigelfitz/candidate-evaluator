{
  "_metadata": {
    "description": "GPT Configuration for Candidate Insights Generation",
    "last_updated": "2025-12-26 13:12:22 UTC",
    "warning": "Changes to these settings affect both quality and cost. Test with small batches first."
  },
  "gpt_model": {
    "value": "gpt-4o",
    "description": "OpenAI model to use for generating candidate insights",
    "options": [
      "gpt-4o",
      "gpt-4o-mini",
      "gpt-4-turbo",
      "gpt-4"
    ],
    "explanation": "gpt-4o: Best balance of quality and cost (RECOMMENDED). gpt-4o-mini: 60% cheaper but lower quality. gpt-4-turbo: Same cost as gpt-4o, slightly different outputs. gpt-4: More expensive, no benefit for this task.",
    "cost_impact": "gpt-4o: $2.50 per 1M input tokens, $10 per 1M output. gpt-4o-mini: $0.15/$0.60. gpt-4-turbo: $10/$30. gpt-4: $30/$60.",
    "quality_impact": "gpt-4o provides personalized, specific insights. gpt-4o-mini may be more generic. gpt-4/turbo offer no quality advantage here."
  },
  "temperature": {
    "value": 0.2,
    "description": "Controls randomness in GPT responses (0.0 = deterministic, 2.0 = very creative)",
    "range": [
      0.0,
      2.0
    ],
    "explanation": "Lower values (0.0-0.3) produce consistent, focused insights. Higher values (0.7-1.0) produce more varied, creative responses. For professional hiring insights, keep LOW.",
    "recommended": 0.2,
    "cost_impact": "No cost impact - only affects output style",
    "quality_impact": "0.2 ensures consistent, professional insights. Higher temps may introduce unnecessary creativity or inconsistency."
  },
  "presence_penalty": {
    "value": 0.4,
    "description": "Reduces repetitive phrasing across insights (0.0 = no penalty, 2.0 = max penalty)",
    "range": [
      0.0,
      2.0
    ],
    "explanation": "Penalizes using the same phrases repeatedly. Higher values = more unique phrasing per candidate. Helps avoid 'demonstrates strong...' being used for everyone.",
    "recommended": 0.4,
    "cost_impact": "No cost impact - only affects output style",
    "quality_impact": "0.3-0.6 produces more personalized, varied insights. Too high (>1.0) may produce awkward phrasing."
  },
  "frequency_penalty": {
    "value": 0.3,
    "description": "Discourages repeating the same words within a response (0.0 = no penalty, 2.0 = max penalty)",
    "range": [
      0.0,
      2.0
    ],
    "explanation": "Reduces word repetition within each insight. Prevents 'Python experience... experienced with Python... Python skills' redundancy.",
    "recommended": 0.3,
    "cost_impact": "No cost impact - only affects output style",
    "quality_impact": "0.2-0.4 produces more natural, varied language. Too high (>0.8) may lose important keyword repetition."
  },
  "max_tokens": {
    "value": 1000,
    "description": "Maximum number of tokens (words) in GPT response",
    "range": [
      500,
      2000
    ],
    "explanation": "Current insights need ~400-600 tokens (3-6 bullets for strengths, 3-6 for gaps, 2-4 sentence notes). 1000 provides safety margin without waste.",
    "recommended": 1000,
    "cost_impact": "Linear cost increase. 1000 tokens = ~$0.01 per insight with gpt-4o. 2000 tokens = ~$0.02. We typically only use ~500.",
    "quality_impact": "Too low (<500) may truncate insights. Too high (>1500) wastes money with no benefit."
  },
  "evidence_snippet_chars": {
    "value": 600,
    "description": "Characters per criterion in evidence snippets sent to GPT",
    "range": [
      200,
      1000
    ],
    "explanation": "Each criterion gets a snippet of resume text showing why the candidate scored well/poorly. More chars = better context but higher cost.",
    "recommended": 600,
    "cost_impact": "With 20 criteria: 400 chars = ~4000 input tokens. 600 chars = ~5000 tokens (+$0.002 per insight). 1000 chars = ~8000 tokens (+$0.010).",
    "quality_impact": "400 chars: Often cuts mid-sentence. 600 chars: Good context, complete thoughts. 1000 chars: Diminishing returns - GPT gets enough from 600."
  },
  "candidate_text_chars": {
    "value": 4000,
    "description": "Characters of candidate resume text sent to GPT for context",
    "range": [
      1000,
      10000
    ],
    "explanation": "Full resume is sent (truncated to this limit) to give GPT overall context about candidate. First 3000 chars usually captures name, summary, recent experience.",
    "recommended": 3000,
    "cost_impact": "3000 chars = ~750 tokens. 5000 chars = ~1250 tokens (+$0.001). 10000 chars = ~2500 tokens (+$0.004).",
    "quality_impact": "3000 chars captures critical info (header, summary, recent roles). More doesn't significantly improve insights since evidence snippets provide specifics."
  },
  "jd_text_chars": {
    "value": 4000,
    "description": "Characters of Job Description sent to GPT for context",
    "range": [
      1000,
      10000
    ],
    "explanation": "Job description is sent (truncated) so GPT understands what role we're hiring for. First 3000 chars captures overview and key requirements.",
    "recommended": 3000,
    "cost_impact": "Same as candidate_text_chars - 3000 = ~750 tokens. Linear increase with length.",
    "quality_impact": "3000 chars sufficient for most JDs. Longer JDs don't improve insights much since criteria are explicitly scored."
  },
  "score_thresholds": {
    "high_threshold": {
      "value": 0.75,
      "description": "Score above this = 'High' or 'Strong' match (green in UI)",
      "range": [
        0.6,
        0.9
      ],
      "explanation": "Criteria with scores \u00e2\u2030\u00a50.75 are considered strong matches. Lower threshold = more criteria marked 'strong'. Higher = more selective.",
      "recommended": 0.75,
      "quality_impact": "Affects UI color coding and what GPT considers a 'strength'. Too low dilutes meaning of 'strong'."
    },
    "low_threshold": {
      "value": 0.35,
      "description": "Score below this = 'Low' or 'Weak' match (red in UI)",
      "range": [
        0.2,
        0.5
      ],
      "explanation": "Criteria with scores <0.35 are considered weak/gaps. Lower threshold = fewer marked as gaps. Higher = more gaps shown.",
      "recommended": 0.35,
      "quality_impact": "Affects UI color coding and what GPT considers a 'gap'. Too high creates false negatives."
    }
  },
  "pricing_config": {
    "base_price_usd": {
      "value": 4.0,
      "description": "Base price for analysis (JD extraction + scoring all candidates + top 3 insights)",
      "explanation": "This covers OpenAI costs for JD extraction (~$0.05) + semantic scoring (~$0.01 per candidate) + 3 insights (~$0.054). Rest is profit margin."
    },
    "extra_insight_price_usd": {
      "value": 1.0,
      "description": "Price per additional insight beyond top 3",
      "explanation": "Each extra insight costs ~$0.018 in OpenAI fees. $1.00 provides 98% profit margin while staying affordable."
    },
    "model_costs": {
      "gpt-4o": {
        "input_per_million": 2.5,
        "output_per_million": 10.0
      },
      "gpt-4o-mini": {
        "input_per_million": 0.15,
        "output_per_million": 0.6
      },
      "gpt-4-turbo": {
        "input_per_million": 10.0,
        "output_per_million": 30.0
      },
      "gpt-4": {
        "input_per_million": 30.0,
        "output_per_million": 60.0
      }
    }
  },
  "advanced_settings": {
    "prompt_style": {
      "value": "concise_professional",
      "description": "Style of insights generated",
      "options": [
        "concise_professional",
        "detailed_analytical",
        "bullet_points_only"
      ],
      "explanation": "concise_professional: 3-6 bullets + paragraph (CURRENT). detailed_analytical: Longer, more verbose. bullet_points_only: Skip the notes paragraph.",
      "note": "Changing this requires modifying the prompt template in analysis.py - not yet implemented in admin UI."
    },
    "include_candidate_name_in_prompt": {
      "value": true,
      "description": "Whether to include candidate name in GPT prompt",
      "explanation": "Including the name helps GPT personalize insights ('Nigel has...' vs 'The candidate has...'). Recommended: true.",
      "quality_impact": "Significantly improves personalization and readability of insights."
    },
    "top_evidence_items": {
      "value": 10,
      "description": "Number of criteria evidence snippets to include (sorted by score)",
      "range": [
        5,
        20
      ],
      "explanation": "We send top N criteria by score to GPT. More = better context but higher cost. 10 balances both.",
      "recommended": 10,
      "cost_impact": "Directly multiplies evidence_snippet_chars cost. 10 items Ã— 600 chars = 6000 chars. 20 items = 12000 chars (double cost)."
    },
    "min_strengths": {
      "value": 3,
      "description": "Minimum number of strengths to generate",
      "range": [
        2,
        8
      ],
      "explanation": "GPT will generate at least this many strength items. Ensures consistent output length.",
      "recommended": 3,
      "quality_impact": "Prevents too-short insights. Most candidates have 3-6 notable strengths."
    },
    "max_strengths": {
      "value": 6,
      "description": "Maximum number of strengths to generate",
      "range": [
        4,
        10
      ],
      "explanation": "GPT will generate at most this many strength items. Prevents overly long insights.",
      "recommended": 6,
      "quality_impact": "Keeps insights focused on most important points. Too many dilutes impact."
    },
    "min_gaps": {
      "value": 3,
      "description": "Minimum number of gaps/concerns to generate",
      "range": [
        2,
        8
      ],
      "explanation": "GPT will generate at least this many gap items. Ensures balanced assessment.",
      "recommended": 3,
      "quality_impact": "Prevents overly positive assessments. Every candidate has areas of concern."
    },
    "max_gaps": {
      "value": 6,
      "description": "Maximum number of gaps/concerns to generate",
      "range": [
        4,
        10
      ],
      "explanation": "GPT will generate at most this many gap items. Prevents overly negative assessments.",
      "recommended": 6,
      "quality_impact": "Keeps assessment constructive. Too many gaps can seem unfairly critical."
    },
    "notes_length": {
      "value": "medium",
      "description": "Length of overall assessment notes",
      "options": [
        "brief",
        "medium",
        "detailed"
      ],
      "explanation": "brief: 1-2 sentences. medium: 2-4 sentences (CURRENT). detailed: 1 paragraph (4-6 sentences).",
      "recommended": "medium",
      "quality_impact": "Medium provides good context without overwhelming. Brief for quick screening, detailed for final rounds."
    },
    "insight_tone": {
      "value": "professional",
      "description": "Tone/style of insights",
      "options": [
        "professional",
        "conversational",
        "technical",
        "executive"
      ],
      "explanation": "professional: Standard hiring language (CURRENT). conversational: More casual/friendly. technical: Focus on tech details. executive: High-level strategic focus.",
      "recommended": "professional",
      "quality_impact": "Match tone to audience. Technical for eng managers, executive for C-suite roles."
    },
    "evidence_filter_strategy": {
      "value": "top_n_by_score",
      "description": "How to select which criteria evidence to send to GPT",
      "options": [
        "top_n_by_score",
        "strengths_and_gaps",
        "threshold_based",
        "all_criteria"
      ],
      "explanation": "top_n_by_score: Send top N highest scores (CURRENT). strengths_and_gaps: Top 5 + Bottom 5. threshold_based: Only >0.65 or <0.40. all_criteria: Send everything.",
      "recommended": "top_n_by_score",
      "cost_impact": "all_criteria doubles token costs. threshold_based is most efficient. top_n provides good balance.",
      "quality_impact": "strengths_and_gaps gives most balanced insights. threshold_based focuses on extremes."
    },
    "evidence_score_threshold_high": {
      "value": 0.65,
      "description": "When using threshold_based strategy, scores above this are sent as strengths",
      "range": [
        0.5,
        0.85
      ],
      "explanation": "Only used if evidence_filter_strategy = 'threshold_based'. Criteria scoring above this are sent to GPT.",
      "recommended": 0.65
    },
    "evidence_score_threshold_low": {
      "value": 0.40,
      "description": "When using threshold_based strategy, scores below this are sent as gaps",
      "range": [
        0.15,
        0.50
      ],
      "explanation": "Only used if evidence_filter_strategy = 'threshold_based'. Criteria scoring below this are sent to GPT.",
      "recommended": 0.40
    },
    "chunk_overlap_chars": {
      "value": 150,
      "description": "Overlap between text chunks when processing long documents",
      "range": [
        50,
        300
      ],
      "explanation": "When splitting resumes into chunks, this many characters overlap between adjacent chunks. Prevents missing context at boundaries.",
      "recommended": 150,
      "quality_impact": "100-200 chars provides good context preservation. Too low may split sentences poorly. Too high wastes processing."
    }
  },
  "_usage_notes": {
    "current_cost_per_insight": "~$0.018 with current settings (gpt-4o, 600 char evidence, 3000 char context)",
    "recommended_workflow": "1. Test changes with 'Top 3' insights first. 2. Check quality in Insights page. 3. Monitor costs in analysis history. 4. Adjust gradually.",
    "optimization_priority": "1. Keep model=gpt-4o (best quality/cost). 2. Adjust evidence_snippet_chars if quality issues (600-800 range). 3. Keep temperature low (0.1-0.3). 4. Don't touch candidate/jd text limits unless necessary.",
    "cost_saving_tips": "Switch to gpt-4o-mini for 60% cost reduction (test quality first). Reduce evidence snippets to 400-500 chars. Lower top_evidence_items to 8.",
    "quality_improvement_tips": "Increase evidence_snippet_chars to 800. Include more evidence items (15-20). Keep temperature at 0.2. Ensure candidate_text includes full resume header."
  }
}